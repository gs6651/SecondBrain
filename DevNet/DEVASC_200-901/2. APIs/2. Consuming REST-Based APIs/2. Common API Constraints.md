
# Common API Constraints

A REST API endpoint can rarely be consumed just by making an HTTP request toward it. Commonly, the API consumer has to identify itself with various authentication and authorization techniques, take into account any availability issues and potential service outages, and sometimes also needs to specify additional parameters to get the content it requires. It all depends on the design of a particular REST API, though, so using good practices when implementing an API is important.

API endpoints are commonly used to return various lists of data. These lists can be fairly short or incredibly long. HTTP responses that are used to return this data can be limited in size by the server, so the amount of data that can be returned with a single API call is not unlimited. This fact creates a need to split up the returned content into more manageable pieces. Also, larger responses take more time to get to the destination or to be processed, which is not something you want to handle if you can avoid it.

Imagine you have an API endpoint, called /users, that returns a list of (several thousand) users for a set of services. An HTTP GET request toward that endpoint might return all the users in a single response.

The following example shows how content can be broken up by using URL parameters by setting the page size as a limit and the page number as the offset. This technique is called offset pagination and is very simple to implement, because it mostly involves safely passing those parameters to the Structured Query Language (SQL) query as shown.

![alt text](/DevNet/DEVASC_200-901/Images/image-330.png)

As you can see, this type of pagination is stateless, because all the data needed is contained in the request. It becomes less useful for larger data sets, because an offset of 1 million rows would mean the query will comb through 1 million rows before finding the data it needs.

Sometimes, the API takes care of pagination by itself and provides links to relevant pages in a response. These links commonly contain references to itself, the next page, and previous page. If the reference to the next page in those cases is either empty or null, you can presume you are currently viewing the last page.

![alt text](/DevNet/DEVASC_200-901/Images/image-331.png)

Not all APIs will use pagination, though. It depends on the implementation of a specific API. In the case of Cisco, pagination is a part of the Cisco Web API standards and is implemented with RFC 5988.

A concept that expands on pagination is *filtering* the content, which also makes use of URL parameters. Similarly, content can also be sorted with parameters.

The following examples show how content can be retrieved from an API by a specific parameter or by a condition, or how it can be sorted in a specific way.

![alt text](/DevNet/DEVASC_200-901/Images/image-332.png)

All these operations save the time and processing power of the client consuming the API. If the client was in this case receiving all the users every single time and then parsing the results client-side, the process would be pretty inefficient, especially with larger data sets.

Ideally, your API should be running as smooth and stable as possible. This is generally true if your API was designed well, has enough resources to handle all the client requests, and there is no malicious activity happening. However, when there is a sudden increase in an API usage, or someone is intentionally trying to make the API misbehave, a mechanism is needed to slow down the number of requests being made.

This technique is called rate limiting. It effectively limits the number of specific requests that can be fulfilled by an API. This number varies depending on the use of the API or even between the different endpoints.

Rate limiting can be implemented either on the server-side or on the client-side.

![alt text](/DevNet/DEVASC_200-901/Images/image-333.png)

Client-side rate limiting is usually implemented in the client application and limits the client from performing a large number of tasks that are costly for the API itself (with a timer, loading bar, or something similar). Server-side limiting is somewhat more effective, because client-side rate limiting is useful only if the API is used as it was intended. Limiting API calls on the server side can prevent denial of service (DoS) attacks that intend to disable the API by flooding it with a huge number of requests. It can also prevent misuse of sensitive or destructive API calls; for example, while renaming multiple resources is a normal operation, rapidly deleting multiple resources might be a sign that the specific client has been compromised. Sometimes, API rates are limited (or throttled) simply because the number of users has increased a lot in a short amount of time and the API needs to be scaled up before it is running at optimal efficiency again.

Requests are commonly limited on a per-user basis (for example, each user can create 1000 API calls per day), and it is not unusual to see API vendors offering premium packets, which increase the daily allowance for that user. Requests also are usually limited per second for security reasons (DoS). Some APIs are also limited regionally; for example, a native webstore located in Italy can get suspicious, if there is a sudden surge of API calls originating from a non-Italian speaking country.

API calls can be throttled with:

- **HTTP headers:** Headers like X-RateLimit-Limit and X-RateLimit-Remaining are used to keep track of the number of used and remaining API calls for a period of time.
- **Message queues:** Incoming API calls can be put into a queue, which makes sure the API endpoint itself is not overloaded.
- **Software libraries and algorithms:** Many libraries and algorithms have been created for the purpose of rate limiting, such as leaky bucket, fixed window, and Sliding Log.
- **Reverse proxies and load balancers:** Load balancers and reverse proxies (like NGINX) feature rate limiting as a built-in feature.

When the API call limit has been reached for a particular user, the user should be notified of that, using a 429 Too Many Requests HTTP response header. This response commonly comes with a Retry-After header that tells the client how long they have to wait before trying again.

Even though APIs are expected to have a high uptime, their service will sometimes be unavailable to the client. Possible causes include the API being overloaded with requests, the network connection between the client and the server experiencing difficulties, some updates being performed on that system, and so on. Instead of just making an API call, it makes sense to expect these kind of problems by checking the status (or health) of the API. This check can be done by sending an HTTP request to a specific API endpoint (something like /api/status), which replies with a lightweight response.

In case the API is down, you might get an HTTP response from the 4xx or 5xx category (most commonly, 404 Not Found or 500 Internal Server Error).

If an error happens, it also makes sense to wait for a short while instead of immediately retrying the API call, because most errors on the server might not be resolved instantaneously.

The following figure is a very basic snippet of a code that a client might use to check the API health.

![alt text](/DevNet/DEVASC_200-901/Images/image-334.png)

Here is a simplified example where the client retries reading users 10 times with 30-second intervals in between. The client checks for a response for HTTP code 200 each try, because it means that the request was successful and has not timed out.

Note that even though you are making a request toward the API endpoint named "users," it does not guarantee that the returned response indeed contains the users object. Response content should always be double-checked and not just inferred.

![alt text](/DevNet/DEVASC_200-901/Images/image-335.png)

> **Note** \
> The code requires extra libraries and will be covered later.

There are several different approaches to the length of the timeouts. The most popular are linear timeout (for example, try it every 60 seconds) or exponential backoff (for example, try it after 1, 2, 4, 8, 16, or 32 seconds, up to a certain maximum).

Timeouts produce HTTP errors like 408 Request Timeout or, if the server is acting only as a proxy, a 504 Gateway Timeout HTTP error. More generic timeouts (for example, the API endpoint is not even connected to the network) may also include the 404 Not Found error.

Some API endpoints (or API gateways that handle the distribution of API calls) that are used for the purpose of uploading data (for example, images on social media platforms or large XML documents) can feature payload limits. By limiting and reducing the size of the body within the HTTP request sent by the client, you reduce the chance of that request being corrupted while transporting, increase the speed with which it can be processed, and limit the damage that a malicious user can inflict by abusing large request bodies (for example, forcing the server to go out of memory, because requests are kept in memory).

![alt text](/DevNet/DEVASC_200-901/Images/image-336.png)

If a payload limit is exceeded, the server should return a 413 Request Entity Too Large error.
